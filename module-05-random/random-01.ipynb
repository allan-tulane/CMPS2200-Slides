{"cells":[{"metadata":{"code_folding":[],"slideshow":{"slide_type":"skip"},"trusted":false},"cell_type":"code","source":"# setup\nfrom IPython.core.display import display,HTML\ndisplay(HTML('<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>'))\ndisplay(HTML(open('rise.css').read()))\n\n# imports\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nsns.set(style=\"whitegrid\", font_scale=1.5, rc={'figure.figsize':(12, 6)})\n","execution_count":4,"outputs":[{"data":{"text/html":"<style>.prompt{width: 0px; min-width: 0px; visibility: collapse}</style>","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"},{"data":{"text/html":"<style>\ndiv#notebook {\n font-family: \"Exo_2\", sans-serif;\n}\n\n.rendered_html h1,\n.text_cell_render h1 {\n color: #126dce;\n font-size: 220%;\n text-align: center;\n font-weight: lighter;\n}\n.rendered_html h2,\n.text_cell_render h2 {\n text-align: center;\n font-size: 170%;\n color: #126dce;\n font-style: normal;\n font-weight: lighter;\n}\n.rendered_html h3,\n.text_cell_render h3 {\n font-size: 150%;\n color: #126dce;\n font-weight: lighter;\n text-decoration: italic;\n font-style: normal;\n}\n.rendered_html h4,\n.text_cell_render h4 {\n font-size: 120%;\n color: #126dce;\n font-weight: underline;\n font-style: normal;\n}\n.rendered_html h5,\n.text_cell_render h5 {\n font-size: 100%;\n color: #2f2f2f;\n font-weight: lighter;\n text-decoration: underline;\n}\n</style>\n","text/plain":"<IPython.core.display.HTML object>"},"metadata":{},"output_type":"display_data"}]},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"# CMPS 2200\n# Introduction to Algorithms\n\n## Randomized Algorithms\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Today's agenda:\n\n- Random variables and expectations\n- Making random choices in algorithms\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"Suppose you could flip a coin to make a decision in an algorithm, rather than relying on inputs. So:\n\n`if (random([0,1]) > 0):\n    print('heads')\nelse:\n    print('tails')`\n\nWhy would we ever do this?"},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"When we don't know exactly how to proceed in solving a problem, we can make a random choice and hope we made the right one.\n\nAlternately, we can view randomization as helping us avoid always making the wrong choice. \n\nAdditionally for instances where we might not have a good way to proceed, we can \"guess\" an answer and \"hope\" it's a good choice. \n\nWhat does \"hope\" mean, computationally? A proof that at least most of the time, we are correct/efficient.\n\n<br><br>\nRandomized algorithms can often be:\n\n1- easier to understand\n\n2- faster\n\n3- more robust to adversarial input (cryptography)"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Basic Definitions\n\nA probability space consists of a **sample space** $\\Omega$ representing the set of possible *outcomes*, and a **probability measure** $\\mathbf{P}: \\Omega \\rightarrow \\mathbb{R}$. The probability measure $\\mathbf{P}$ must satisfy: </p>\n\n- **Nonnegativity**: $\\mathbf{P}\\left[{A}\\right] \\in [0,1]$\n\n- **Additivity**: For any two disjoint events $A$ and $B$ (i.e., $A \\cap B = \\emptyset$): $\\mathbf{P}\\left[{A \\cup B}\\right] =  \n  \\mathbf{P}\\left[{A}\\right] + \\mathbf{P}\\left[{B}\\right]$\n\n- **Normalization**: $\\mathbf{P}\\left[{\\Omega}\\right] = 1$\n\nFor example, let $\\Omega$ be the set of all outcomes of a pair of 6-sided dice.\n\nHow many outcomes are there if the dice sum to 3? To 4?\n\nThe dice sum to 3 if we roll any of $\\{(1, 2), (2, 1)\\}$, and sum to 4 if we roll any of $\\{(1, 3), (2, 2), (3, 1)\\}$.\n\nEvery outcome is disjoint and has probability 1/36. So the probability of rolling a 3 is 2/36 = 1/18 and the probability of rolling a 4 is 3/36 = 1/12.\n\nIf we want to consider multiple, possibly overlapping events. Then, the **union bound** is helpful:\n\n$$\\mathbf{P}\\left[{\\bigcup_{0 \\leq i < n} A_i}\\right] \\leq \\sum_{i=0}^{n-1} \\mathbf{P}\\left[{A_i}\\right]$$\n\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Conditional Probabilities\n\nWe may want to know the probability of an event $A$ with \"partial knowledge.\"  The **conditional probability** of an event $A$ *given* that $B$ occurs ($\\mathbf{P}\\left[{B}\\right] > 0$) is \n\n$$\\mathbf{P}\\left[{A \\mid B}\\right] = \\frac{\\mathbf{P}\\left[{A \\cap B}\\right]}{\\mathbf{P}\\left[{B}\\right]}$$\n\nFor example, what is the probability of rolling a sum of 6 given that the first die is a 1? 2? \n\n"},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"A useful fact that relates conditional probabilities to non-conditional probabilities is the **Law of Total Probability**:\n\nLet $\\Omega$ be a sample space and let $A_0, \\ldots, A_{n-1}$ be a set of events that partition  $\\Omega$ such that $\\mathbf{P}\\left[{A_i}\\right] > 0$ for all $0 \\le i < n.$ Then, for any event $B$ we have that: \n    \n$\\begin{array}{lll}  \n\\mathbf{P}\\left[{B}\\right]  & = & \\displaystyle\\sum_{i=0}^{n-1} \\mathbf{P}\\left[{B \\cap A_i}\\right]  \n\\\\  \n& = &  \\sum_{i=0}^{n-1} \\mathbf{P}\\left[{A_i}\\right]\\mathbf{P}\\left[{B \\mid A_i}\\right]  \n\\end{array}$\n\nAn intuitive way to think about this is that the probability of an event $B$ is just the probability over different possible (disjoint) situations. \n\nAnother useful concept is **independence**. We say that two events $A$ and $B$ are independent when they are disjoint. So if $A$ and $B$ are independent, $\\mathbf{P}\\left[{A \\cap B}\\right] = \\mathbf{P}\\left[{A}\\right] \\cdot \\mathbf{P}\\left[{B}\\right].$\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Random Variables and Expectations\n\nA **random variable** $X$ is a real-valued function on the outcomes of an experiment, i.e. $X : \\Omega\\to \\mathbb{R}$. We will consider only discrete random variables.\n\nEven more simply, we will most often use **indicator random variables**, which map outcomes to 0 or 1. \n\nNote: The term \"random variable\" is somewhat odd. Really it's just a function, but we use this terminology since it's useful to look at functions of outcomes (which have probabilities under a given probability measure). \n\n\n"},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"The **expectation** of a random variable $X$ defined for a probability space $(\\Omega, \\mathbf{P})$ is denoted:\n\n$$\\mathbf{E}_{\\Omega,\\mathbf{P}}[X] = \\sum_{y \\in \\Omega} X(y) \\cdot  \n\\mathbf{P}\\left[\\{y\\}\\right].$$\n\nWe usually write this more conveniently as:\n\n$$ \\mathbf{E}\\left[{X}\\right] = \\sum_{x} x \\cdot \\mathbf{P}\\left[X = x\\right]. $$\n\nLet $X$ be the value of a pair of dice. $X$ can take on values between 2 and 12, where each has a certain probability based on the possible ways the dice can sum to that value.  What is the expected value of the value of a pair of unbiased dice? It is:\n\n\n$\\begin{eqnarray*}\n\\mathbf{E}\\left[{X}\\right] &=& \\sum_{x=2}^{12} x \\cdot \\mathbf{P}\\left[X = x\\right] \\\\\n&=& 2\\cdot\\frac{1}{36} + 3\\cdot\\frac{2}{36} + 4\\cdot\\frac{3}{36} + \\cdots +7\\cdot\\frac{6}{36}+ \\cdots  + 11\\cdot\\frac{2}{36} + 12\\cdot\\frac{1}{36}\\\\\n& = & 7\n\\end{eqnarray*}$\n\n<p><br><br>\nIntuitively, the expectation is really just a weighted average of the values of the random variable.\n    \nLet's look at coin flips where we have equal probability of heads/tails. Let $X$ be the number of flips until we get heads. What is $E[X]$? Observe that:\n\n$\\begin{eqnarray*}\nE[X] &=& \\frac{1}{2}\\cdot 1 + \\frac{1}{2} (1 + E[X]) \\\\\n\\end{eqnarray*}$\n\nSolving for $E[X]$, we get that $E[X] = 2$. More generally, the expected number of trials to get an outcome of probability $p$ is $1/p$. (We'll see this later.)\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"We will make repeated use of some basic facts about the expectations of random variables.\n\nFirst, we can easily treat the expected value of functions of random variables:\n    \n$$ \\mathbf{E}\\left[f(X)\\right] = \\sum_{x} f(x) \\cdot \\mathbf{P}\\left[X = x\\right]. $$\n\nWe also have **linearity of expectations** for random variables:\n\n$$ \\mathbf{E}\\left[X + Y\\right] = \\mathbf{E}\\left[X\\right] + E\\left[ Y\\right]. $$\n\nThis identity will help us simply the calcuation of the *expected* work and span of an algorithm. \n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"How does all this relate to algorithms?\n\nThe sample space we must consider for an algorithm is the set of decisions made by the algorithm. We typically use random variables that map outcomes to a way to measure work/span.\n\n\n<br><br>\n\nHowever it is usually difficult to reason directly about the overall outcome (e.g. correctness or work/span), so we try to identify when choices are independent or conditional upon one another.\n\n<br><br>\n\nUltimately if we choose to make random choices in our algorithm, we must relax our notion of correctness and our definitions of work/span.\n\n\n<br><br>\n\n\nFor correctness, some executions may produce the wrong output while others are correct. We usually seek to have a very high *probability of correctness*. \n\n<br><br>\n\n\nFor work/span, some executions may take longer than others. We seek to provide asymptotic bounds on the *expected* work or span of a randomized algorithm. Intuitively, this is the weighted average of the running times over all possible sets of random choices. \n\nWhen using randomization we usually have one of two types of algorithms.\n\nA **Monte Carlo** randomized algorithm has a **deterministic** worst-case runtime but a randomized output that is correct with some probability.\n\nA **Las Vegas** randomized algorithm always produces a correct solution, but has an **expected** runtime.  \n\nWe will focus on Las Vegas algorithms."},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"\n### Randomly splitting a list\n\nSuppose we are given a list $L$ of length $n$. We choose a random index $i$ of the list and return $L[i:]$ as output. \n\nWhat is the expected size of $L[i:]$? \n"},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"\nLet $X$ be a random variable that is the size of the output list. By the definition of expectation we have:\n\n$\\begin{eqnarray*}\n\\mathbf{E}[X] &=& \\sum_{x=1}^{n} x \\cdot \\mathbf{P}[X = x] \\\\\n&=& \\sum_{x=1}^{n} x \\cdot \\frac{1}{n} \\\\\n&=& \\frac{1}{n}\\cdot\\sum_{x=1}^{n} x \\\\\n&=& \\frac{1}{n}\\cdot\\frac{n(n+1)}{2}\\\\\n&=& \\frac{n+1}{2} \\\\\n\\end{eqnarray*}$\n\nThis might follow your intuition that, since all indices are equally likely, the chosen index averages out to roughly half of the list size. \n\nA related question is, what is the probability of getting a list with more than half of the elements? That is, what is $\\mathbf{P}\\left[X \\geq n/2\\right]$?\n\n$\\begin{eqnarray*}\n\\mathbf{P}\\left[X \\geq n/2\\right] &=& \\sum_{x=n/2}^{n} \\mathbf{P}[X = x] \\\\\n&=& \\sum_{x=n/2}^{n} \\frac{1}{n} \\\\\n&=& \\frac{1}{n}\\cdot\\sum_{x=n/2}^{n} 1 \\\\\n&=& \\frac{n/2}{n} \\\\\n&=& \\frac{1}{2} \\\\\n\\end{eqnarray*}$\n\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"### Randomly filtering elements\n\nSuppose we are given a list of length $L$ of length $n$. \n\nFor each element we flip an unbiased coin $x_i$. Return a list $R$ with $L[i]$ such that $x_i$ is heads.\n\nWhat is the expected size of $R$?\n"},{"metadata":{"slideshow":{"slide_type":"fragment"}},"cell_type":"markdown","source":"Let $X_i$ be an indicator random variable that is 1 if element $i$ is chosen, and 0 otherwise. Let $X$ be the size of $R$. We can see that $X = \\sum_{i=0}^{n-1} X_i$ by definition. So by linearity of expectation we can compute:\n\n$$\\mathbf{E}[X] = \\sum_{i=0}^{n=1} \\mathbf{E}[X_i].$$\n\nNow notice that by the definition of expectation \n\n$$E[X_i] = 0\\cdot \\mathbf{P}[X_i = 0] + 1\\cdot\\mathbf{P}[X_i = 1] = \\mathbf{P}[X_i = 1].$$\n\nSince we flip a coin for each element independently, $\\mathbf{P}[X_i = 1] = 1/2.$ Thus, $\\mathbf{E}[X_i] = 1/2$ for all $i$. So we have that\n\n$$\\mathbf{E}[X] = \\sum_{i=0}^{n=1} \\frac{1}{2} = \\frac{n}{2}.$$\n\n\nSo the expected size of $R$ is $n/2$.\n"},{"metadata":{"slideshow":{"slide_type":"slide"}},"cell_type":"markdown","source":"These simplistic algorithms illustrate the kind of analysis we will be doing to analyze the expected work and span of algorithms for manipulating lists. \n\nWe will look at algorithms for selection and sorting. For both problems we will analyze recursive algorithms that use randomization to select the input for the recursive calls. \n\nThis creates a complicated situation in which each successive call depends on the previous one, and thereby requires some clever accounting to derive the expected work and span."}],"metadata":{"celltoolbar":"Slideshow","kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"rise":{"autolaunch":true,"controls":false,"enable_chalkboard":true,"scroll":true,"theme":"simple","transition":"fade"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":true,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"nbformat":4,"nbformat_minor":4}