{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recitation-04\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "#### PART ONE ###\n",
    "\n",
    "def run_map_reduce(map_f, reduce_f, docs):\n",
    "    # done. do not change me.\n",
    "    \"\"\"    \n",
    "    The main map reduce logic.\n",
    "    \n",
    "    Params:\n",
    "      map_f......the mapping function\n",
    "      reduce_f...the reduce function\n",
    "      docs.......list of input records\n",
    "    \"\"\"\n",
    "    # 1. call map_f on each element of docs and flatten the results\n",
    "    # e.g., [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1), ('sam', 1), ('is', 1), ('ham', 1)]\n",
    "    pairs = flatten(list(map(map_f, docs)))\n",
    "    # 2. group all pairs by their key\n",
    "    # e.g., [('am', [1, 1]), ('ham', [1]), ('i', [1, 1]), ('is', [1]), ('sam', [1, 1])]\n",
    "    groups = collect(pairs)\n",
    "    # 3. reduce each group to the final answer\n",
    "    # e.g., [('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n",
    "    return [reduce_f(g) for g in groups]\n",
    "\n",
    "def word_count_map(doc):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      doc....a string to be split into tokens. split on whitespace.\n",
    "    Returns:\n",
    "      a list of tuples of form (token, 1), where token is a whitespace delimited element of this string.\n",
    "      \n",
    "    E.g.\n",
    "    >>> word_count_map('i am sam i am')\n",
    "    [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    return [(token, 1) for token in doc.split()]\n",
    "    ###\n",
    "\n",
    "def test_word_count_map():\n",
    "    assert word_count_map('i am sam i am') == \\\n",
    "           [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1)]\n",
    "\n",
    "def word_count_reduce(group):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      group...a tuple of the form (token, list_of_ones), indicating the number of times each word appears.\n",
    "    Returns:\n",
    "      tuple of form (token, int), where int is the number of times that token appears\n",
    "    E.g.\n",
    "    >>> word_count_reduce(['i', [1,1]])\n",
    "    ('i', 2)\n",
    "    \n",
    "    NOTE: you should use call the `reduce` function here.\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    return (group[0], reduce(plus, 0, group[1]))\n",
    "    ###\n",
    "    \n",
    "def test_word_count_reduce():\n",
    "    assert word_count_reduce(['i', [1,1,1]]) == ('i', 3)\n",
    "\n",
    "def test_word_count():\n",
    "    assert run_map_reduce(word_count_map, word_count_reduce, ['i am sam i am', 'sam is ham']) == \\\n",
    "           [('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n",
    "\n",
    "def iterate(f, x, a):\n",
    "    # done. do not change me.\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      f.....function to apply\n",
    "      x.....return when a is empty\n",
    "      a.....input sequence\n",
    "    \"\"\"\n",
    "    if len(a) == 0:\n",
    "        return x\n",
    "    else:\n",
    "        return iterate(f, f(x, a[0]), a[1:])\n",
    "    \n",
    "def flatten(sequences):\n",
    "    # done. do not change me.\n",
    "    return iterate(plus, [], sequences)\n",
    "\n",
    "def collect(pairs):\n",
    "    \"\"\"\n",
    "    # done. do not change me.\n",
    "    Implements the collect function (see text Vol II Ch2)\n",
    "    E.g.:\n",
    "    >>> collect([('i', 1), ('am', 1), ('sam', 1), ('i', 1)])\n",
    "    [('am', [1]), ('i', [1, 1]), ('sam', [1])]    \n",
    "    \"\"\"\n",
    "    result = defaultdict(list)\n",
    "    for pair in sorted(pairs):\n",
    "        result[pair[0]].append(pair[1])\n",
    "    return list(result.items())\n",
    "\n",
    "\n",
    "def plus(x, y):\n",
    "    # done. do not change me.\n",
    "    return x + y\n",
    "\n",
    "def reduce(f, id_, a):\n",
    "    # done. do not change me.\n",
    "    if len(a) == 0:\n",
    "        return id_\n",
    "    elif len(a) == 1:\n",
    "        return a[0]\n",
    "    else:\n",
    "        return f(reduce(f, id_, a[:len(a)//2]),\n",
    "                 reduce(f, id_, a[len(a)//2:]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "### PART TWO ###\n",
    "\n",
    "def sentiment_map(doc,\n",
    "                  pos_terms=set(['good', 'great', 'awesome', 'sockdolager']),\n",
    "                  neg_terms=set(['bad', 'terrible', 'waste', 'carbuncle', 'corrupted'])):\n",
    "    \"\"\"\n",
    "    Params:\n",
    "      doc.........a string to be split into tokens. split on whitespace.\n",
    "      pos_terms...a set of positive terms\n",
    "      neg_terms...a set of negative terms\n",
    "    Returns:\n",
    "      a list of tuples of form (positive, 1) or (negative, 1)      \n",
    "    E.g.\n",
    "    >>> sentiment_map('it was a terrible waste of time')\n",
    "    [('negative', 1), ('negative', 1)]\n",
    "    \"\"\"\n",
    "    ###TODO\n",
    "    res = []\n",
    "    for token in doc.split():\n",
    "        if token in pos_terms:\n",
    "            res.append(('positive', 1))\n",
    "        elif token in neg_terms:\n",
    "            res.append(('negative', 1))\n",
    "    return res\n",
    "    ###\n",
    "\n",
    "def test_sentiment_map():\n",
    "    assert sentiment_map('it was a terrible waste of time') == [('negative', 1), ('negative', 1)]\n",
    "\n",
    "    \n",
    "def test_sentiment():\n",
    "    docs = [\n",
    "        'it was not great but not terrible',\n",
    "        'thou art a boil a plague-sore or embossed carbuncle in my corrupted blood',\n",
    "        'it was a sockdolager of a good time'\n",
    "    ]\n",
    "    result = run_map_reduce(sentiment_map, word_count_reduce, docs)\n",
    "    assert result == [('negative', 3), ('positive', 3)]\n",
    "\n",
    "\n",
    "    \n",
    "test_word_count_map()\n",
    "test_word_count_reduce()\n",
    "test_word_count()\n",
    "test_sentiment_map()\n",
    "test_sentiment()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
