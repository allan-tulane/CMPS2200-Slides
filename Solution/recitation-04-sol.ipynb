{"cells":[{"cell_type":"markdown","source":["\n","## Part 1: Counting Words\n","\n","In the first part, we will use map-reduce to count how often each word appears in a sequence of documents. E.g. if the input is two documents:\n","\n","```python\n","['i am sam i am', 'sam is ham']\n","```\n","\n","then the output should be\n","\n","```python\n","[('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n","```\n","\n","We have given you the implementation of the main map-reduce logic\n","```python\n","def run_map_reduce(map_f, reduce_f, docs)\n","```\n","\n","To use this function to count words, you'll need to implement your own `map_f` and `reduce_f` functions, described below.\n","\n","1. Complete `word_count_map` and test it with `test_word_count_map`. Please use doc.split() to split a string. \n","\n","2. Complete `word_count_reduce` and test it with `test_word_count_reduce`.\n","\n","3. If the above are correct, then you should now be able to test it the full solution `test_word_count`\n","\n","4. Assume that a word `w` appears `n` times. What is the **work** and **span** of `word_count_reduce` for this word, assuming a parallel implementation of the `reduce` function?\n","\n","**Enter answer here**\n","\n"," - **Solution** $O(n)$ for work; $O(\\log n)$ for span.\n","\n","\n","5. Why are we going through all this trouble? Couldn't I just use this function to count words?\n","\n","```python\n","docs = ['i am sam i am', 'sam is ham']\n","counts = {}\n","for doc in docs:\n","    for term in doc.split():\n","        counts[term] = counts.get(term, 0) + 1\n","# counts = {'i': 2, 'am': 2, 'sam': 2, 'is': 1, 'ham': 1}\n","```\n","\n","What is the problem that prevents us from easily parallelizing this solution?\n","\n","**Enter answer here**\n","\n"," - **Solution** This one is sequential without any chance to do parellel computing. If there are n docments with n sequences per each, there are 2 for loops, the work and span are both $O(n^2)$"],"metadata":{"id":"XMOf22i_TldD"}},{"cell_type":"code","source":["# Recitation-04\n","\n","## Given Pre-Defined Functions\n","from collections import defaultdict\n","\n","\n","def iterate(f, x, a):\n","    # done. do not change me.\n","    \"\"\"\n","    Params:\n","      f.....function to apply\n","      x.....return when a is empty\n","      a.....input sequence\n","    \"\"\"\n","    if len(a) == 0:\n","        return x\n","    else:\n","        return iterate(f, f(x, a[0]), a[1:])\n","    \n","def flatten(sequences):\n","    # done. do not change me.\n","    return iterate(plus, [], sequences)\n","\n","def collect(pairs):\n","    \"\"\"\n","    # done. do not change me.\n","    Implements the collect function (see text Vol II Ch2)\n","    E.g.:\n","    >>> collect([('i', 1), ('am', 1), ('sam', 1), ('i', 1)])\n","    [('am', [1]), ('i', [1, 1]), ('sam', [1])]    \n","    \"\"\"\n","    result = defaultdict(list)\n","    for pair in sorted(pairs):\n","        result[pair[0]].append(pair[1])\n","    return list(result.items())\n","\n","\n","def plus(x, y):\n","    # done. do not change me.\n","    return x + y\n","\n","def reduce(f, id_, a):\n","    # done. do not change me.\n","    if len(a) == 0:\n","        return id_\n","    elif len(a) == 1:\n","        return a[0]\n","    else:\n","        return f(reduce(f, id_, a[:len(a)//2]),\n","                 reduce(f, id_, a[len(a)//2:]))\n","    \n","    "],"metadata":{"id":"1z-1qBHrT3BY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#### PART ONE ###\n","\n","def run_map_reduce(map_f, reduce_f, docs):\n","    # done. do not change me.\n","    \"\"\"    \n","    The main map reduce logic.\n","    \n","    Params:\n","      map_f......the mapping function\n","      reduce_f...the reduce function\n","      docs.......list of input records\n","    \"\"\"\n","    # 1. call map_f on each element of docs and flatten the results\n","    # e.g., [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1), ('sam', 1), ('is', 1), ('ham', 1)]\n","    pairs = flatten(list(map(map_f, docs))) \n","    # 2. group all pairs by their key\n","    # e.g., [('am', [1, 1]), ('ham', [1]), ('i', [1, 1]), ('is', [1]), ('sam', [1, 1])]\n","    groups = collect(pairs)\n","    # 3. reduce each group to the final answer\n","    # e.g., [('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n","    return [reduce_f(g) for g in groups]\n","\n","def word_count_map(doc):\n","    \"\"\"\n","    Params:\n","      doc....a string to be split into tokens. split on whitespace.\n","    Returns:\n","      a list of tuples of form (token, 1), where token is a whitespace delimited element of this string.\n","      \n","    E.g.\n","    >>> word_count_map('i am sam i am')\n","    [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1)]\n","    \"\"\"\n","    ###TODO\n","    return [(token, 1) for token in doc.split()]\n","    ###\n","\n","def test_word_count_map():\n","    assert word_count_map('i am sam i am') == \\\n","           [('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1)]\n","\n","## Test 1\n","print(word_count_map('i am sam i am'))\n","\n","def word_count_reduce(group):\n","    \"\"\"\n","    Params:\n","      group...a tuple of the form (token, list_of_ones), indicating the number of times each word appears.\n","    Returns:\n","      tuple of form (token, int), where int is the number of times that token appears\n","    E.g.\n","    >>> word_count_reduce(['i', [1,1]])\n","    ('i', 2)\n","    \n","    NOTE: you should use call the `reduce` function here.\n","    \"\"\"\n","    ###TODO\n","    return (group[0], reduce(plus, 0, group[1]))\n","    ###\n","    \n","def test_word_count_reduce():\n","    assert word_count_reduce(['i', [1,1,1]]) == ('i', 3)\n","\n","## Test 2\n","\n","print(word_count_reduce(['i', [1,1,1]]))\n","\n","def test_word_count():\n","    assert run_map_reduce(word_count_map, word_count_reduce, ['i am sam i am', 'sam is ham']) == \\\n","           [('am', 2), ('ham', 1), ('i', 2), ('is', 1), ('sam', 2)]\n","\n","\n","    \n","    \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GE0KFLgsTdZW","executionInfo":{"status":"ok","timestamp":1647611950144,"user_tz":300,"elapsed":354,"user":{"displayName":"Allan Z. Ding","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiWd9rkd0Uun_yaQDwm7o7CWumY58ZYYKGPidK96JA=s64","userId":"12421824806550707843"}},"outputId":"55cb1f46-af9e-427d-af7a-4b364ce023b7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('i', 1), ('am', 1), ('sam', 1), ('i', 1), ('am', 1)]\n","('i', 3)\n"]}]},{"cell_type":"markdown","source":["## Part 2: Sentiment analysis\n","\n","Finally, we'll adapt our approach above to perform a simple type of sentiment analysis. Given a document, rather than counting words, we will instead count the number of positive and negative terms in the document, given a predefined list of terms. E.g., if the input sentence is `it was a terrible waste of time` and the terms `terrible` and `waste` are in our list of negative terms, then the output is\n","\n","`[('negative', 1), ('negative', 1)]`\n","\n","6. Complete the `sentiment_map` function to implement the above idea and test it with `test_sentiment_map`.\n","\n","7. Since the output here is similar to the word count problem, we will reuse `word_count_reduce` to compute the total number of positive and negative terms in a sequence of documents. Confirm your results work by running `test_sentiment`.\n"],"metadata":{"id":"ZghqLKKVURYg"}},{"cell_type":"code","source":["### PART TWO ###\n","\n","def sentiment_map(doc,\n","                  pos_terms=set(['good', 'great', 'awesome', 'sockdolager']),\n","                  neg_terms=set(['bad', 'terrible', 'waste', 'carbuncle', 'corrupted'])):\n","    \"\"\"\n","    Params:\n","      doc.........a string to be split into tokens. split on whitespace.\n","      pos_terms...a set of positive terms\n","      neg_terms...a set of negative terms\n","    Returns:\n","      a list of tuples of form (positive, 1) or (negative, 1)      \n","    E.g.\n","    >>> sentiment_map('it was a terrible waste of time')\n","    [('negative', 1), ('negative', 1)]\n","    \"\"\"\n","    ###TODO\n","    res = []\n","    for token in doc.split():\n","        if token in pos_terms:\n","            res.append(('positive', 1))\n","        elif token in neg_terms:\n","            res.append(('negative', 1))\n","    return res\n","    ###\n","\n","def test_sentiment_map():\n","    assert sentiment_map('it was a terrible waste of time') == [('negative', 1), ('negative', 1)]\n","\n","\n","## Test\n","print(sentiment_map('it was a terrible waste of time'))\n","\n","\n","def test_sentiment():\n","    docs = [\n","        'it was not great but not terrible',\n","        'thou art a boil a plague-sore or embossed carbuncle in my corrupted blood',\n","        'it was a sockdolager of a good time'\n","    ]\n","    result = run_map_reduce(sentiment_map, word_count_reduce, docs)\n","    assert result == [('negative', 3), ('positive', 3)]\n","\n","\n","    "],"metadata":{"id":"WJ78bLYPTelQ"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"name":"recitation-04-sol.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}